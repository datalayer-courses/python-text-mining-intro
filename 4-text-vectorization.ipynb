{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is based on:\n",
    "- https://openclassrooms.com/en/courses/6532301-introduction-to-natural-language-processing/6980811-apply-a-simple-bag-of-words-approach\n",
    "- https://openclassrooms.com/en/courses/6532301-introduction-to-natural-language-processing/7067116-apply-the-tf-idf-vectorization-approach\n",
    "- https://openclassrooms.com/en/courses/6532301-introduction-to-natural-language-processing/6980911-discover-the-power-of-word-embeddings\n",
    "- https://www.shanelynn.ie/word-embeddings-in-python-with-spacy-and-gensim/\n",
    "- https://ashutoshtripathi.com/2020/09/04/word2vec-and-semantic-similarity-using-spacy-nlp-spacy-series-part-7/\n",
    "- https://israelg99.github.io/2017-03-23-Word2Vec-Explained/\n",
    "\n",
    "# 4. Text vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization is the general process of turning a collection of text documents (a corpus) into numerical feature vectors fed to machine learning algorithms for modeling. When you vectorize the corpus, you convert each word or token from the documents into an array of numbers. This array is the vector representation of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-words (BOW) is a simple but powerful approach to vectorizing text.\n",
    "\n",
    "As the name may suggest, the bag-of-words technique does not consider the position of a word in a document. The idea is to count the number of times each word appears in each of the documents. It is a simple method, but it works.\n",
    "\n",
    "Consider the three following documents and count the number of times each word appears in each sentence.\n",
    "\n",
    "<img src=\"https://user.oc-static.com/upload/2020/10/23/16034397439042_surfin%20bird%20bow.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "The matrix calculated on this simple example of three sentences can be generalized to many documents in the corpus. Each document is a row, and each token is a column. Such a matrix is called the document-term matrix.\n",
    "\n",
    "Note that the size of the document-term matrix is:\n",
    "number of documents ∗ size of vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the CountVectorizer from scikit-learn (you can read more on the official documentation page https://scikit-learn.org/stable/modules/feature_extraction.html) to generate the document-term matrix from a corpus with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 2, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "    '2 cups of flour',\n",
    "    'replace the flour',\n",
    "    'replace the keyboard in 2 minutes',\n",
    "    'do you prefer Windows or Mac',\n",
    "    'the Mac has the most noisy keyboard',\n",
    "]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row corresponds to one of the sentences, and each column to a word in the corpus. For instance, the appears once in documents two and three and twice in document five, while the word flour appears once in documents one and two. The vocabulary is strongly related to the sentence topic: the word flour only appears in documents about recipes. On the other hand, the is less specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the size of the vocabulary is important to avoid performing calculations over gigantic matrices. While removing stop words and lemmatizing helps reduce the size of the vocabulary significantly, it's often not enough.\n",
    "Therefore, reducing the size of the vocabulary is crucial. The idea is to remove as many tokens as possible without throwing away relevant information. It's a delicate balance that is entirely dependent on the context. One strategy can be to filter out words that are either too frequent or too rare. Another strategy involves applying dimension reduction techniques (PCA) to the document-term matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with counting word occurrences is that some words appear only in a limited number of documents. The model will learn that pattern, overfit the training data, and fail to generalize to new texts properly. Similarly, words that are present in all the documents will not bring any information to the model.\n",
    "\n",
    "For this reason, it is sometimes better to normalize the word counts by the number of times they appear in the documents. This is the general idea behind the tf-idf vectorization.\n",
    "\n",
    "Let's look more closely at what tf-idf stands for:\n",
    "\n",
    "- Tf stands for term frequency, the number of times the word appears in each document. \n",
    "- Idf stands for inverse document frequency, an inverse count of the number of documents a word appears in. Idf measures how significant a word is in the whole corpus.\n",
    "\n",
    "If you multiply tf with idf, you get the tf-idf score: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf(t,d)=|Number of times term t appears in document d|\n",
    "\n",
    "idf(t,D)=|Number of documents|/| number of documents that contain term t|\n",
    "\n",
    "tfidf(t,d,D)=tf(t,d).idf(t,D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- t is the word or token.\n",
    "- d is the document.\n",
    "- D is the set of documents in the corpus.\n",
    "\n",
    "A tf-idf score is a decimal number that measures the importance of a word in any document. It gives small values to frequent words in all the documents and more weight to those more scarce across the corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, tf-Idf is implemented as the  TfidfVectorizer (you can read more on the scikit-learn documentation https://scikit-learn.org/stable/modules/feature_extraction.html).\n",
    "\n",
    "There are multiple ways to calculate the tf or the idf part of tf-idf depending on whether you want to maximize the impact of rare words or lower the role of frequent ones. For instance, when the corpus is composed of documents of varying length, you can normalize by the size of each document:\n",
    "\n",
    "tf(t,d)=nt,d / Vocab size of the document d\n",
    "Take the log:\n",
    "\n",
    "tf(t,d)=log(nt,d+1)\n",
    "\n",
    "Here, nt,d is the number of times the term t appears in the document d.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the  CountVectorizer over the TfidfVectorizer depends on the nature of the documents you are working on. Tf-idf does not always bring better results than merely counting the occurrences of the words in each document!\n",
    "\n",
    "Here are a couple of cases where tf could perform better than tf-idf :\n",
    "\n",
    "- If words are distributed equally across the documents, then normalizing by idf will not matter much. As such, taking into account each word's specificity across the corpus does not improve the model's performance.\n",
    "- If rare words do not carry valuable meaning to the classification model, then td-idf does not have a particular advantage. For example, when someone uses slang, that means something general in a comment on social media. \n",
    "\n",
    "By concatenating each document's scores in the corpus, you get a vector. The dimension of the word vector equals the number of documents in the corpus. For example, if the corpus holds four documents, the vector's dimension is 4. For a corpus of 1000 documents, the vector dimension is 1000. Words that are not in the corpus do not get a vector representation, meaning that the vocabulary size and elements are also entirely dependent on the corpus at play. In short, tf-idf vectorization gives a numerical representation of words entirely dependent on the nature and number of documents being considered. The same words will have different vector representations in another corpus.\n",
    "\n",
    "We will explore numerical representations of words called embeddings (Word2vec, GloVe, fastText) the next part. These techniques are absolute and not dependent on the corpus, which is an important distinction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Words embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that the bag-of-words and tf-idf approaches are simple and quite efficient methods, but have several shortcomings:\n",
    "- Context and meaning is lost.\n",
    "- The document-term matrix is large and sparse.\n",
    "- Vectorization is relative to the corpus (similar words will have different vectors on another corpus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2013, a new text vectorizing method called embeddings took NLP by storm. An embedding technique called Word2vec was born, soon to be followed by GloVe and fastText. These new text vectorization techniques solved the inherent shortcomings of bag of words and tf-idf approaches. They also somehow managed to retain semantic similarity between words, meaning that these vectors can recognize the meaning of a word and determine its similarity to others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages \n",
    "\n",
    "#### They Retain Semantic Similarity\n",
    "\n",
    "As mentioned, one of the most remarkable properties of embeddings is their ability to capture the semantic relationship between words. For example: A hammer and pliers are both tools. Since they are related or similar in meaning, their vectors will be near one another. Similar to the words apple and pear or truck and vehicle.\n",
    "\n",
    "When visualizing word vectors in a 2D space, similar words are grouped in the same regions. The figure below shows the five most similar words: Paris, London, Moscow, Twitter, Facebook, pizza, fish, train, and car, according to Word2vec embeddings.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://user.oc-static.com/upload/2021/01/11/16103734115528_P3C1-1%20%281%29.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "As you can see, similar words keep their semantic distance! Truly amazing!\n",
    "\n",
    "With embeddings, it also becomes possible to capture analogies between words. For example, a woman is to a queen what a man is to a king; Paris is to France what Berlin is to Germany. You can also add and subtract words.\n",
    "\n",
    "queen−→−−−−woman−→−−−−=king−→−−man−→−France−→−−−−−Paris−→−−=Germany−→−−−−−−Berlin−→−−−  \n",
    "\n",
    "In this case, the distance between the respective vectors for woman and queen is close to the distance between the vectors for man and king."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### They Have Dense Vectors\n",
    "Word embeddings are dense vectors, meaning that all values are non-zero (except for the occasional element). Therefore, more information is given to the model, leading to better performances.\n",
    "\n",
    "#### They Have a Constant Vector Size\n",
    "With word embeddings, the vector size is no longer dependent on the number of documents in your corpus!\n",
    "\n",
    "When training embedding models, the dimension of the word vector is a parameter of the model. You decide beforehand what vector size you need to represent each word. Pre-trained embeddings usually come in dimensions 50, 100, and 300.\n",
    "\n",
    "#### Their Vector Representations are Absolute\n",
    "The vector representations are also independent on the nature and content of the corpus you are working on.\n",
    "\n",
    "Word embeddings are trained on gigantic datasets. Word2vec, for instance, was trained on a Google News dataset of 100 billion words, GloVe on a dataset of 6 billion words, and fastText on 16 billion tokens. As a direct consequence, these models have very large vector representations. Word2vec has 3 million vectors, GloVe has 400.000, and fastText has 1 million vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained models in Spacy\n",
    "\n",
    "Spacy has a number of different models of different sizes available for use, with models in 7 different languages (include English, Polish, German, Spanish, Portuguese, French, Italian, and Dutch), and of different sizes to suit your requirements. en_core_web_md includes 20k unique vectors with 300 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the spacy model that you have installed\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men_core_web_md\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Process a sentence using the model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is some text that I am processing with Spacy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\spacy\\__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     31\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     38\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\spacy\\util.py:436\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 436\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spacy model that you have installed\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Process a sentence using the model\n",
    "doc = nlp(\"This is some text that I am processing with Spacy\")\n",
    "\n",
    "# It's that simple - all of the vectors and words are assigned after this point\n",
    "# Get the vector for 'text':\n",
    "doc[3].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectors can be accessed directly using the .vector attribute of each processed token (word). The mean vector for the entire sentence is also calculated simply using .vector, providing a very convenient input for machine learning models based on sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the mean vector for the entire sentence (useful for sentence classification etc.)\n",
    "doc.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic similarity can be also illustrated as followed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp('lions cat pet')\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec explained\n",
    "\n",
    "Word2Vec is a shallow, two-layer neural networks which is trained to reconstruct linguistic contexts of words.\n",
    "It takes as its input a large corpus of words and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space.\n",
    "Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.\n",
    "Word2Vec is a particularly computationally-efficient predictive model for learning word embeddings from raw text.\n",
    "It comes in two flavors, the Continuous Bag-of-Words (CBOW) model and the Skip-Gram model.\n",
    "Algorithmically, these models are similar.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://israelg99.github.io/images/2017-03-23-Word2Vec-Explained/word2vec_diagrams.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW predicts target words (e.g. ‘mat’) from the surrounding context words (‘the cat sits on the’).\n",
    "Statistically, it has the effect that CBOW smoothes over a lot of the distributional information (by treating an entire context as one observation). For the most part, this turns out to be a useful thing for smaller datasets.\n",
    "\n",
    "Skip-gram predicts surrounding context words from the target words (inverse of CBOW).\n",
    "Statistically, skip-gram treats each context-target pair as a new observation, and this tends to do better when we have larger datasets.\n",
    "\n",
    "Word2Vec is a simple neural network with a single hidden layer, and like all neural networks, it has weights, and during training, its goal is to adjust those weights to reduce a loss function. However, Word2Vec is not going to be used for the task it was trained on, instead, we will just take its hidden weights, use them as our word embeddings, and toss the rest of the model.\n",
    "\n",
    "The rows of the hidden layer weight matrix, are actually the word vectors (word embeddings) we want!\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"https://israelg99.github.io/images/2017-03-23-Word2Vec-Explained/word2vec_weight_matrix_lookup_table.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above explanation is a very basic one. It just gives you a high-level idea of what word embeddings are and how Word2Vec works. There’s a lot more to it. For example, to make the algorithm computationally more efficient, tricks like Hierarchical Softmax and Skip-Gram Negative Sampling are used. Moreover, GloVe, which extends the work of Word2Vec to capture global contextual information in a text corpus by calculating a global word-word co-occurrence matrix, and FastText, which works with sub-word tokenization and, as a consequence, can handle out-of-vocabulary words, are also worth looking at. Finally, Google’s Bidirectional Encoder Representations from Transformer (BERT), which became the highlight by the end of 2018 for achieving state-of-the-art performance in many NLP tasks, can also be used for word's embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
